{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "\n",
    "## SECTION: Transfer learning\n",
    "\n",
    "### LECTURE: Transfer learning with ResNet18\n",
    "\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anura\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for importing data\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IBSQO5HB6Kje"
   },
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HOkOefftqyg"
   },
   "source": [
    "# Import a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R1huHlhc4gnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data\\stl10_binary.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 293240832/2640397119 [09:34<1:20:49, 484004.33it/s]"
     ]
    }
   ],
   "source": [
    "### Note: resnet is trained for images in a specific range (NOT [-1,1]).\n",
    "#         That changes the mean/std normalization values in the transform.\n",
    "\n",
    "# transformations\n",
    "transform = T.Compose([ T.ToTensor(), # normalizes to range [0,1]\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) # further normalization\n",
    "                       ])\n",
    "\n",
    "# import the data and simultaneously apply the transform\n",
    "trainset = torchvision.datasets.STL10(root='./data', download=True, split='train', transform=transform)\n",
    "testset  = torchvision.datasets.STL10(root='./data', download=True, split='test',  transform=transform)\n",
    "\n",
    "# transform to dataloaders\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(trainset,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(testset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYwGTs2bxIjU"
   },
   "outputs": [],
   "source": [
    "# check out the shape of the datasets\n",
    "print('Data shapes (train/test):')\n",
    "print( trainset.data.shape )\n",
    "print( testset.data.shape )\n",
    "\n",
    "# and the range of pixel intensity values\n",
    "print('\\nData value range:')\n",
    "print( (np.min(trainset.data),np.max(trainset.data)) )\n",
    "\n",
    "# the unique categories\n",
    "print('\\nData categories:')\n",
    "print( trainset.classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ4PjMSr4hPo"
   },
   "outputs": [],
   "source": [
    "# Uh oh! It looks like the images are the wrong dimensions!\n",
    "# They need to be 3x96x96\n",
    "# And they are not normalized!\n",
    "\n",
    "# but...\n",
    "X,y = next(iter(train_loader))\n",
    "\n",
    "# try again\n",
    "print('Data shapes (train/test):')\n",
    "print( X.data.shape )\n",
    "\n",
    "# and the range of pixel intensity values\n",
    "print('\\nData value range:')\n",
    "print( (torch.min(X.data),torch.max(X.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk_A-XJAismr"
   },
   "outputs": [],
   "source": [
    "# histogram of the data\n",
    "plt.hist(X.data.numpy().flatten(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMpyFzF-d95B"
   },
   "outputs": [],
   "source": [
    "# inspect a few random images\n",
    "\n",
    "fig,axs = plt.subplots(4,4,figsize=(10,10))\n",
    "\n",
    "for (i,ax) in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract that image (need to transpose it back to 32x32x3)\n",
    "  pic = X.data[i].numpy().transpose((1,2,0))\n",
    "  pic = pic-np.min(pic) # undo normalization\n",
    "  pic = pic/np.max(pic)\n",
    "  \n",
    "  # and its label\n",
    "  label = trainset.classes[y[i]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic)\n",
    "  ax.text(0,0,label,ha='left',va='top',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXVi3dSWIGK2"
   },
   "source": [
    "# Import and inspect the resnet model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLIQkY3WIlGX"
   },
   "outputs": [],
   "source": [
    "# The following line was recorded in the video, but is now depreciated. See also Q&A.\n",
    "# resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# You can use the following instead. \n",
    "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "resnet = torchvision.models.resnet18(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7jb3M2LIlJ3"
   },
   "outputs": [],
   "source": [
    "# let's inspect this network\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miH6N6-nIlNI"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(resnet.to(device),(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DmEk8JqSsJM"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers (final layer changed later)\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "    # python note: the above operation can be implemented in-line:\n",
    "    #p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVbQK-B-KlQS"
   },
   "outputs": [],
   "source": [
    "# change the final layer\n",
    "resnet.fc = nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLj5EhFJLcPa"
   },
   "outputs": [],
   "source": [
    "# push the model to the GPU (if using)\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTt-fBI_IlP_"
   },
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WRSghzBIlTB"
   },
   "outputs": [],
   "source": [
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(),lr=0.001,momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTU176WWIlV3"
   },
   "outputs": [],
   "source": [
    "numepochs = 10\n",
    "\n",
    "# initialize losses\n",
    "trainLoss = torch.zeros(numepochs)\n",
    "testLoss  = torch.zeros(numepochs)\n",
    "trainAcc  = torch.zeros(numepochs)\n",
    "testAcc   = torch.zeros(numepochs)\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # loop over training data batches\n",
    "  resnet.train() # switch to train mode\n",
    "  batchLoss = []\n",
    "  batchAcc  = []\n",
    "  for X,y in train_loader:\n",
    "\n",
    "    # push data to GPU\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat = resnet(X)\n",
    "    loss = lossfun(yHat,y)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # loss and accuracy from this batch\n",
    "    batchLoss.append(loss.item())\n",
    "    batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "  # end of batch loop...\n",
    "\n",
    "  # and get average losses and accuracies across the batches\n",
    "  trainLoss[epochi] = np.mean(batchLoss)\n",
    "  trainAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "\n",
    "  #### test performance (here done in batches!)\n",
    "  resnet.eval() # switch to test mode\n",
    "  batchAcc  = []\n",
    "  batchLoss = []\n",
    "  for X,y in test_loader:\n",
    "\n",
    "    # push data to GPU\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # forward pass and loss\n",
    "    with torch.no_grad():\n",
    "      yHat = resnet(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "    \n",
    "    # loss and accuracy from this batch\n",
    "    batchLoss.append(loss.item())\n",
    "    batchAcc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "  # end of batch loop...\n",
    "\n",
    "  # and get average losses and accuracies across the batches\n",
    "  testLoss[epochi] = np.mean(batchLoss)\n",
    "  testAcc[epochi]  = 100*np.mean(batchAcc)\n",
    "\n",
    "  # print out a status update\n",
    "  print(f'Finished epoch {epochi+1}/{numepochs}. Test accuracy = {testAcc[epochi]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPrWLULcL5ne"
   },
   "source": [
    "# Visualize the performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubv4l86NIlYw"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(trainLoss,'s-',label='Train')\n",
    "ax[0].plot(testLoss,'o-',label='Test')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss (MSE)')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(trainAcc,'s-',label='Train')\n",
    "ax[1].plot(testAcc,'o-',label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_title(f'Final model train/test accuracy: {trainAcc[-1]:.2f}/{testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.suptitle('Pretrained ResNet-18 on STL10 data',fontweight='bold',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4KF0H8K6L_vm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.datasets' has no attribute 'stl10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound GPU at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load and preprocess the STL10 dataset\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m (train_images, train_labels), (test_images, test_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstl10\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Normalize the images\u001b[39;00m\n\u001b[0;32m     17\u001b[0m train_images \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.datasets' has no attribute 'stl10'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, datasets, applications, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU setup\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print(f'Found GPU at: {device_name}')\n",
    "\n",
    "# Load and preprocess the STL10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.stl10.load_data()\n",
    "\n",
    "# Normalize the images\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Mean and std normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "train_images = (train_images - mean) / std\n",
    "test_images = (test_images - mean) / std\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "# Create a ResNet-18 model\n",
    "resnet = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "# Freeze the layers of the pretrained model\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the new model\n",
    "model = models.Sequential([\n",
    "    data_augmentation,\n",
    "    resnet,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=num_epochs, batch_size=batch_size, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Plotting the loss and accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax[0].plot(history.history['loss'], 's-', label='Train')\n",
    "ax[0].plot(history.history['val_loss'], 'o-', label='Test')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], 's-', label='Train')\n",
    "ax[1].plot(history.history['val_accuracy'], 'o-', label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title(f'Final model train/test accuracy: {history.history[\"accuracy\"][-1]:.2f}/{history.history[\"val_accuracy\"][-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.suptitle('Pretrained ResNet-50 on STL10 data', fontweight='bold', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t968l__2IlbY"
   },
   "outputs": [],
   "source": [
    "# inspect a few random images\n",
    "\n",
    "X,y = next(iter(test_loader))\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "resnet.eval()\n",
    "predictions = torch.argmax( resnet(X) ,axis=1)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(4,4,figsize=(10,10))\n",
    "\n",
    "for (i,ax) in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract that image (need to transpose it back to 96x96x3)\n",
    "  pic = X.data[i].cpu().numpy().transpose((1,2,0))\n",
    "  pic = pic-np.min(pic) # undo normalization\n",
    "  pic = pic/np.max(pic)\n",
    "  \n",
    "  # show the image\n",
    "  ax.imshow(pic)\n",
    "  \n",
    "  \n",
    "  # label and true class\n",
    "  label = trainset.classes[predictions[i]]\n",
    "  truec = trainset.classes[y[i]]\n",
    "  title = f'Pred: {label}  -  true: {truec}'\n",
    "\n",
    "  # set the title with color-coded accuracy\n",
    "  titlecolor = 'g' if truec==label else 'r'\n",
    "  ax.text(48,90,title,ha='center',va='top',fontweight='bold',color='k',backgroundcolor=titlecolor,fontsize=8)\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIlAxsUvIld8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwitjEcTPc7y"
   },
   "source": [
    "# Additional explorations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4YRCUgHVmi3"
   },
   "outputs": [],
   "source": [
    "# 1) Try re-downloading the resnet18, unfreeze the layers, and re-run. This means you'll be fine-tuning the entire\n",
    "#    network instead of only the final prediction layer.\n",
    "# \n",
    "# 2) Download an untrained resnet18. This is simply the architecture with random weights (you'll still need to replace\n",
    "#    the final layer so it has 10 outputs). Train this model; how is the performance?\n",
    "# \n",
    "# 3) I used SGD as the backprop method. Try re-running the analysis using Adam. Does this help or hurt the train and\n",
    "#    test performance?\n",
    "# \n",
    "# 4) ~80% accuracy is pretty decent considering we didn't do anything to optimize the model. Looking through the model\n",
    "#    metaparameters, what are some things you would try to change if you wanted to boost performance?\n",
    "# \n",
    "# 5) You've seen earlier in the course that data normalization is important. This is particularly so for pretrained \n",
    "#    networks, because the weights are tuned to specific numerical ranges. But how important is the *exact* numerical \n",
    "#    range? To find out, re-run the code but remove the normalization transform. Thus, the images now will be in the \n",
    "#    range [0,1], which is overlapping with but smaller than (and non-negative) the range that the network is trained on.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/IwwL7Grqig5dCKbJ32U2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
